---
title: "DATA_621_HW1"
author: "Chi Pong, Euclid Zhang, Jie Zou, Joseph Connolly, LeTicia Cancel"
date: "1/30/2022"
output:
  html_document:
    df_print: paged
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

```{r include=FALSE}
library("corrplot")
library("MASS")
#library("car")
```

```{r}
train_raw_df <- read.csv("https://raw.githubusercontent.com/ezaccountz/DATA_621/main/HW1/moneyball-training-data.csv")
test_raw_df <- read.csv("https://raw.githubusercontent.com/ezaccountz/DATA_621/main/HW1/moneyball-evaluation-data.csv")

train_raw_df$INDEX <- NULL
test_raw_df$INDEX <- NULL
```

# DATA EXPLORATION

```{r}
summary(train_raw_df)
```

## **Outliers**

From the summaries The maximum values for TEAM_PITCHING_H, TEAM_PITCHING_BB, TEAM_PITCHING_SO and TEAM_FIELDING_E seem abnormally large. There may be outliers in the columns. We can confirm this finding by checking the distributions of the values:

```{r fig.height=10, fig.width=10}
par(mfrow=c(4,4))
for(i in c(1:16)) {
  boxplot(train_raw_df[,i],main=colnames(train_raw_df)[i])
}
```

From the boxplots, there are indeed values in TEAM_PITCHING_H, TEAM_PITCHING_BB, TEAM_PITCHING_SO and TEAM_FIELDING_E that are extremly off from the majority of the data. We would handle these outliers later with the other problems.

## **Missing values**

From the summaries, we see that are are missing values for TEAM_BATTING_SO, TEAM_BASERUN_SB, TEAM_BASERUN_CS, EAM_BATTING_HBP, TEAM_PITCHING_SO, and TEAM_FIELDING_DP. Now we check the portion of missing data in each field:

```{r}
sapply(train_raw_df,function(x)sum(is.na(x)))/nrow(train_raw_df)
```

91.6% of the data in TEAM_BATTING_HBP are missing. Since the minimum of TEAM_BATTING_HBP is 29, it is not plausible that the missing values are all 0. We will drop this field from our analysis as there are too many missing values and there is no good way of imputing the values.

For TEAM_BATTING_SO, TEAM_BASERUN_SB, TEAM_BASERUN_CS, TEAM_PITCHING_SO, and TEAM_FIELDING_DP, we will do imputation and may be handled with other problems.

## **Correlations**

Now let's look at the correlations between the variables

```{r fig.height=10, fig.width=10}
corrplot(cor(train_raw_df, use = "na.or.complete"), method = 'number', type = 'lower', diag = FALSE, tl.srt = 0.1)
```

**The following variables are nearly perfectly correlated**

-   TEAM_BATTING_H and TEAM_PITCHING_H
-   TEAM_BATTING_HR and TEAM_PITCHING_HR
-   TEAM_BATTING_BB and TEAM_PITCHING_BB
-   TEAM_BATTING_SO and TEAM_PITCHING_SO

We take a more careful look at the correlation between the TARGET_WINS and other variables, and compare it with the theoretical effects

```{r paged.print=FALSE}
corr_table <- data.frame(correlation_with_TARGET_WINS = round(cor(train_raw_df, use = "na.or.complete"),4)[-1,"TARGET_WINS"])
corr_table$Theoretical_Effect <- c("Positive","Positive","Positive","Positive","Positive","Negative","Positive","Negative","Positive","Negative","Negative","Negative","Positive","Negative","Positive")
corr_table
```

\*\*The following variables do not have correlation matching with the theoretical effect:

-   TEAM_BATTING_3B
-   TEAM_PITCHING_H
-   TEAM_PITCHING_HR
-   TEAM_PITCHING_BB
-   TEAM_PITCHING_SO
-   TEAM_FIELDING_DP

For TEAM_BATTING_3B and TEAM_FIELDING_DP, we would need to perform deeper analysis on this finding.\
For TEAM_PITCHING_H, TEAM_PITCHING_HR, TEAM_PITCHING_BB, TEAM_PITCHING_SO, we may consider dropping the variables since they are amlost perfectly correlated with one other variable. Also, TEAM_PITCHING_H, TEAM_PITCHING_BB and TEAM_PITCHING_SO have outlier problem as found above.

## **Normalities**

We check the normalities of the variables to determine if transformation is needed.

```{r fig.height=10, fig.width=10}
par(mfrow=c(4,4))
for(i in c(1:16)) {
  plot(density(train_raw_df[,i],na.rm=TRUE),main=colnames(train_raw_df)[i])
}
```

TEAM_PITCHING_H, TEAM_PITCHING_BB, TEAM_PITCHING_SO and TEAM_FIELDING_E are largely right skewed because of the outliers. We will check the distributions later again when the outliers are handled.\
TEAM_BATTING_HR, TEAM_BATTING_SO, TEAM_PITCHING_HR are bimodal that may need to be transformed.\
There are some variables such as TEAM_BASERUN_SB that are slightly right skewed. We may keep them as it for easier interpretation of the result.

# DATA PREPARATION

## **Dropping variables**

**TEAM_BATTING_HBP** is dropped for the following reason(s)

-   91.6% of the data are missing

**TEAM_PITCHING_HR** is dropped for the following reason(s)

-   nearly perfectly correlated with one other variable
-   do not have correlation matching with the theoretical effect

**TEAM_PITCHING_H*,*TEAM_PITCHING_BB*,*TEAM_PITCHING_SO** are dropped for the following reason(s)

-   nearly perfectly correlated with one other variable
-   do not have correlation matching with the theoretical effect
-   large outliers

```{r}
train_prepared_df <- train_raw_df
train_prepared_df$TEAM_BATTING_HBP <- NULL
train_prepared_df$TEAM_PITCHING_HR <- NULL
train_prepared_df$TEAM_PITCHING_H <- NULL
train_prepared_df$TEAM_PITCHING_BB <- NULL
train_prepared_df$TEAM_PITCHING_SO <- NULL
```

## **Transforming variables**

Since **TEAM_FIELDING_E** is extremely right skewed, we will transform the variable using Box-Cox transformation

The optimal lamba from the following result plot is near -1, so we will transform the variable using the power of -1

```{r fig.height=4, fig.width=5}
boxcox(lm(train_prepared_df$TEAM_FIELDING_E ~ 1))
```

```{r}
train_prepared_df$TEAM_FIELDING_E_Transformed <- train_prepared_df$TEAM_FIELDING_E^(-1)
train_prepared_df$TEAM_FIELDING_E <- NULL
```

The following density plot and box plot show that the distribution is closer to normal and there are no extreme ourliers.

```{r fig.height=3, fig.width=6}
par(mfrow=c(1,2))
plot(density(train_prepared_df$TEAM_FIELDING_E_Transformed),main="",xlab="")
boxplot(train_prepared_df$TEAM_FIELDING_E_Transformed,main="")
```

Since **TEAM_BATTING_H** also counts the number of **TEAM_BATTING_2B**, **TEAM_BATTING_3B** and **TEAM_BATTING_3B**.\
Instead of using **TEAM_BATTING_H**, we will create a new variable **TEAM_BATTING_1B** by subtracting **TEAM_BATTING_2B**, **TEAM_BATTING_3B** and **TEAM_BATTING_3B** from **TEAM_BATTING_H**

```{r}
train_prepared_df$TEAM_BATTING_1B <- train_prepared_df$TEAM_BATTING_H - train_prepared_df$TEAM_BATTING_2B - train_prepared_df$TEAM_BATTING_3B - train_prepared_df$TEAM_BATTING_HR
train_prepared_df$TEAM_BATTING_H <- NULL
```

The distribution of the new variable is slightly right skewed. We will keep it as it for now unless transformation is necessary when developing a model.

```{r fig.height=3, fig.width=6}
par(mfrow=c(1,2))
plot(density(train_prepared_df$TEAM_BATTING_1B),main="",xlab="")
boxplot(train_prepared_df$TEAM_BATTING_1B,main="")
```

## **Imputation for Missing Values**

The only problem now is the missing values in **TEAM_BATTING_SO**, **TEAM_BASERUN_SB**, **TEAM_BASERUN_CS** and **TEAM_FIELDING_DP**

We will impute the missing values by using linear regression models. We will not go deep into evaluating these models in this project. The purpose here is to impute missing values have better explained variance than simply using the means or medians.

In the imputation models, **TARGET_WINS** is not included as an independent variable since the values are not provided in the test data set. We will need to use the same models to impute missing values in the test data set.

We will perform separate imputations in the following order, based on the number of missing values: 1. **TEAM_BATTING_SO** 2. **TEAM_BASERUN_SB** 3. **TEAM_FIELDING_DP** 4. **TEAM_BASERUN_CS**

Variables that are already imputed are included in the models after and un-imputed variables are not included. We will keep the trained models and use them to impute missing values in the test data set.

Before imputations:

```{r}
missing_df <- data.frame(train_prepared_df$TEAM_BATTING_SO,train_prepared_df$TEAM_BASERUN_SB,train_prepared_df$TEAM_BASERUN_CS, train_prepared_df$TEAM_FIELDING_DP)

(colMeans(is.na(missing_df)))*100
```

```{r}
lm_team_bat_so <- lm(TEAM_BATTING_SO ~ . - TEAM_BASERUN_SB - TEAM_BASERUN_CS - TEAM_FIELDING_DP, data = train_prepared_df[,2:ncol(train_prepared_df)])
train_prepared_df[is.na(train_prepared_df$TEAM_BATTING_SO),]$TEAM_BATTING_SO <- predict(lm_team_bat_so,train_prepared_df[is.na(train_prepared_df$TEAM_BATTING_SO),])

lm_team_bas_sb <- lm(TEAM_BASERUN_SB ~ . - TEAM_BASERUN_CS - TEAM_FIELDING_DP, data = train_prepared_df[,2:ncol(train_prepared_df)])
train_prepared_df[is.na(train_prepared_df$TEAM_BASERUN_SB),]$TEAM_BASERUN_SB <- predict(lm_team_bas_sb,train_prepared_df[is.na(train_prepared_df$TEAM_BASERUN_SB),])

train_prepared_df[train_prepared_df$TEAM_BASERUN_SB<0,]$TEAM_BASERUN_SB <- 0

lm_team_fld_dp <- lm(TEAM_FIELDING_DP ~ . - TEAM_BASERUN_CS, data = train_prepared_df[,2:ncol(train_prepared_df)])
train_prepared_df[is.na(train_prepared_df$TEAM_FIELDING_DP),]$TEAM_FIELDING_DP <- predict(lm_team_fld_dp,train_prepared_df[is.na(train_prepared_df$TEAM_FIELDING_DP),])

lm_team_bas_cs <- lm(TEAM_BASERUN_CS ~ ., data = train_prepared_df[,2:ncol(train_prepared_df)])
train_prepared_df[is.na(train_prepared_df$TEAM_BASERUN_CS),]$TEAM_BASERUN_CS <- predict(lm_team_bas_cs,train_prepared_df[is.na(train_prepared_df$TEAM_BASERUN_CS),])
```

After imputations:

```{r}
# imputed_df <- data.frame(lm_team_bat_so, lm_team_bas_sb, lm_team_fld_dp, lm_team_bas_cs)
# (colMeans(is.na(imputed_df)))*100
missing_df <- data.frame(train_prepared_df$TEAM_BATTING_SO,train_prepared_df$TEAM_BASERUN_SB,train_prepared_df$TEAM_BASERUN_CS, train_prepared_df$TEAM_FIELDING_DP)

(colMeans(is.na(missing_df)))*100

```

As you can all see, the 4 variables, *TEAM_BATTING_SO, TEAM_BASERUN_SB, TEAM_BASERUN_CS, TEAM_FIELDING_DP*, all are having 0 missing values. Meaning they are no longer presented with missing values.

The R-squared values of the models are:

```{r}
print(paste0("TEAM_BATTING_SO - R-squared:",toString(round(summary(lm_team_bat_so)$r.squared,4))))
print(paste0("TEAM_BASERUN_SB - R-squared:",toString(round(summary(lm_team_bas_sb)$r.squared,4))))
print(paste0("TEAM_BASERUN_CS - R-squared:",toString(round(summary(lm_team_bas_cs)$r.squared,4))))
print(paste0("TEAM_FIELDING_DP - R-squared:",toString(round(summary(lm_team_fld_dp)$r.squared,4))))
```

The numbers in the summary of our prepared data set all look plausible, we are ready for our model development

```{r}
summary(train_prepared_df)
```

# BUILD MODELS

## **1. Full model:**

**TEAM_BATTING_1B**, **TEAM_BATTING_2B**, **TEAM_BATTING_3B**, **TEAM_BATTING_HR**, **TEAM_BATTING_BB**, **TEAM_BATTING_SO**, **TEAM_BASERUN_SB**, **TEAM_BASERUN_CS**, **TEAM_FIELDING_DP**, **TEAM_FIELDING_E\_Transformed**

```{r}
lm_win_full <- lm(TARGET_WINS ~ .,data = train_prepared_df)
```

```{r}
summary(lm_win_full)
```

In the full model, The sign of **TEAM_BASERUN_CS** and **TEAM_FIELDING_DP** do not match with the theoretical effects.\
By looking at the below confidence intervals with 5% significance level, The positive sign for **TEAM_BASERUN_CS** may just happen by chance. The high p-value of **TEAM_BASERUN_CS** also indicates that the variable is not significant here. With the fact that more than 30% of the values of **TEAM_BASERUN_CS** are imputed. We suggest to drop this variable from our model.\
For **TEAM_FIELDING_DP**, confidence interval is below 0 and the correlation between **TARGET_WINS** and **TEAM_FIELDING_DP** is -0.2. In the correlation matrix, it doesn't show any strong correlation with any other variables. One explanation is that double play happens when there is already a runner on a base. So a higher **TEAM_FIELDING_DP** means the team let the an opponent player stay on a base more frequently. A good team will get their opponents out before double play can happen. We may need to observe the behaviors of the baseball players directly to find out the true reason of the negative correlation. But, in this analysis, we will compare the performance of the models with or without **TEAM_FIELDING_DP** using the test data set to determine whether we should keep this variable in our model.

```{r}
confint(lm_win_full, level = 0.95)
```

## **2. Adjusted model 1:**

exclude **TEAM_BASERUN_CS**

```{r}
lm_win_eff_adj <- lm(TARGET_WINS ~ .-TEAM_BASERUN_CS,data = train_prepared_df)
```

```{r}
summary(lm_win_eff_adj)
```

The significance of **TEAM_BASERUN_SB** increases and the Adjusted R-squared increases. Dropping **TEAM_BASERUN_CS** gives us a better result. We will also verify this in our test data set.

## **3. Adjusted model 2:**

exclude **TEAM_BASERUN_CS** and **TEAM_FIELDING_DP**

```{r}
lm_win_eff_adj2 <- lm(TARGET_WINS ~ .-TEAM_BASERUN_CS-TEAM_FIELDING_DP,data = train_prepared_df)
```

```{r}
summary(lm_win_eff_adj2)
```

The significance of **TEAM_BATTING_2B** and **TEAM_BATTING_SO** decreases and the Adjusted R-squared also decreases. It seems that it is better to keep **TEAM_FIELDING_DP** in our model.

## **4. Adjusted model 3:**

exclude all variables with imputed values (**TEAM_BATTING_SO**, **TEAM_BASERUN_SB**, **TEAM_BASERUN_CS** and **TEAM_FIELDING_DP**)

```{r}
lm_win_exc_mis <- lm(TARGET_WINS ~ .-TEAM_BATTING_SO-TEAM_BASERUN_SB-TEAM_BASERUN_CS-TEAM_FIELDING_DP,data = train_prepared_df)
```

```{r}
summary(lm_win_exc_mis)
```

As expected, including less variables produce a model with lower R-squared value. The Adjusted R-squared is also lower in this case. However, this model is much simpler and all the variables have no missing values. We will compare the performance of this model to the other ones we have built using the test data set.

# SELECT MODELS

## **Inference Plots**

```{r fig.height=10, fig.width=10}
par(mfrow=c(4,4))

plot(lm_win_full)
plot(lm_win_eff_adj)
plot(lm_win_eff_adj2)
plot(lm_win_exc_mis)
```

The plots for all four models all look the same.

### Constant Variance

\
The residual plots and standardized residual plots show that the residuals are independent and approximately constant with mean 0 within the cloud of data. Basically, we see in all 4 residuals vs fitted plots (Column 1) that there is approximately symmetrical variation around 0, i.e. homoscedasticity. Nor did we detect any non-linear patterns.

\
The Q-Q plots show that the residuals are approximately normal except the two tails but the problem is mild.\
The Residuals vs Leverage plots show no point outside of the Cook's distance. There is no strong influence point.\
We conclude that all four models follow the assumptions of OLS regression and so they are valid.

## **F-Statistic**

The F-statistic is used to measure the significance of one or more variables if they are added to a base model.\
The base model is our model with **TEAM_BATTING_SO**, **TEAM_BASERUN_SB**, **TEAM_BASERUN_CS** and **TEAM_FIELDING_DP** excluded\
We compare it with the model with **TEAM_BATTING_SO** and **TEAM_BASERUN_SB** added

```{r}
anova(lm_win_exc_mis, lm_win_eff_adj2)
```

The F-value with degree of freedom 2 is 31.068, indicating that adding **TEAM_BATTING_SO** and **TEAM_BASERUN_SB** does make an improvement to the model.

Next use our improved model as our new base model check if adding **TEAM_FIELDING_DP** will make another improvement.

```{r}
anova(lm_win_eff_adj2, lm_win_eff_adj)
```

The result shows that the effect of **TEAM_FIELDING_DP** is significant

Last, let's check if adding **TEAM_BASERUN_CS** will help

```{r}
anova(lm_win_eff_adj, lm_win_full)
```

The variable **TEAM_BASERUN_CS** insignificant as we have checked before using the p-value of the coefficient.

From the F-Statistics, the optimal model is the model with the following predictors:

-   TEAM_BATTING_2B
-   TEAM_BATTING_3B
-   TEAM_BATTING_HR
-   TEAM_BATTING_BB
-   TEAM_BATTING_SO
-   TEAM_BASERUN_SB
-   TEAM_FIELDING_DP
-   TEAM_FIELDING_E\_Transformed
-   TEAM_BATTING_1B

## **Adjusted R-squared and RMSD (Root Mean Square Deviation)**

The R-squared or Adjusted R-squared is used to measure how well a model fit in the train data. Since our models have different number of predictor variables, it is better to compare the Adjusted R-squared of the 4 models

The RMSD (Root Mean Square Deviation) is a measurement for the difference between a model's predicted values and the actual values.

The Adjusted R-squared and RMSD for all 4 models are as following:

```{r}

data.frame(
    model = c("Full model","Exclude TEAM_BASERUN_CS","EXclude TEAM_BASERUN_CS and TEAM_FIELDING_DP","Exclude variables with missing values"),     Adjusted_R_Squared = c(summary(lm_win_full)$adj.r.squared,summary(lm_win_eff_adj)$adj.r.squared,summary(lm_win_eff_adj2)$adj.r.squared,summary(lm_win_exc_mis)$adj.r.squared),
    Root_Mean_Square_Deviation = c(sqrt(mean(lm_win_full$residuals^2)),sqrt(mean(lm_win_eff_adj$residuals^2)),sqrt(mean(lm_win_eff_adj2$residuals^2)),sqrt(mean(lm_win_exc_mis$residuals^2)))
    )
```

The second model has the highest Adjusted_R\_Squared value and an RMSD slightly higher than the smallest RMSD produced by the full model. We conclude that the optimal model is the same as we found from the F-Statistic tests.

## **Test data set prediction**

Now, let's check if the models are producing plausible prediction values using the test data set.

let's look at the summary of our test data set:

```{r}
summary(test_raw_df)
```

First, let's transform and impute the variables in the test data set.

```{r}
test_prepared_df <- test_raw_df
test_prepared_df$TEAM_FIELDING_E_Transformed <- test_prepared_df$TEAM_FIELDING_E^(-1)

test_prepared_df$TEAM_BATTING_1B <- test_prepared_df$TEAM_BATTING_H - test_prepared_df$TEAM_BATTING_2B - test_prepared_df$TEAM_BATTING_3B - test_prepared_df$TEAM_BATTING_HR
test_prepared_df$TEAM_BATTING_H <- NULL
  
test_prepared_df$TEAM_BATTING_HBP <- NULL
test_prepared_df$TEAM_PITCHING_HR <- NULL
test_prepared_df$TEAM_PITCHING_H <- NULL
test_prepared_df$TEAM_PITCHING_BB <- NULL
test_prepared_df$TEAM_PITCHING_SO <- NULL
```

```{r}

test_prepared_df[is.na(test_prepared_df$TEAM_BATTING_SO),]$TEAM_BATTING_SO <- predict(lm_team_bat_so,test_prepared_df[is.na(test_prepared_df$TEAM_BATTING_SO),])

test_prepared_df[is.na(test_prepared_df$TEAM_BASERUN_SB),]$TEAM_BASERUN_SB <- predict(lm_team_bas_sb,test_prepared_df[is.na(test_prepared_df$TEAM_BASERUN_SB),])

test_prepared_df[test_prepared_df$TEAM_BASERUN_SB<0,]$TEAM_BASERUN_SB <- 0

test_prepared_df[is.na(test_prepared_df$TEAM_FIELDING_DP),]$TEAM_FIELDING_DP <- predict(lm_team_fld_dp,test_prepared_df[is.na(test_prepared_df$TEAM_FIELDING_DP),])

test_prepared_df[is.na(test_prepared_df$TEAM_BASERUN_CS),]$TEAM_BASERUN_CS <- predict(lm_team_bas_cs,test_prepared_df[is.na(test_prepared_df$TEAM_BASERUN_CS),])
```

The following is the summary of the prepared test data

```{r}
summary(test_prepared_df)
```

The follow plot shows the distribution of the actual Target_Win value from the train data set and the distributions of the predicted values from the 4 models

```{r}
m1_predict <- predict(lm_win_full, test_prepared_df)
m2_predict <- predict(lm_win_eff_adj, test_prepared_df)
m3_predict <- predict(lm_win_eff_adj2, test_prepared_df)
m4_predict <- predict(lm_win_exc_mis, test_prepared_df)
```

```{r}
dist_df <- data.frame(rbind(
      cbind(train_prepared_df$TARGET_WINS,"train data set"),
      cbind(m1_predict,"test data model 1"),
      cbind(m2_predict,"test data model 2"),
      cbind(m3_predict,"test data model 3"),
      cbind(m4_predict,"test data model 4")
      ),stringsAsFactors=FALSE)
colnames(dist_df) <- c("value","data")
dist_df$value <- as.numeric(dist_df$value)
```

```{r}
library(ggplot2)

ggplot(dist_df, aes(x=value, color=data)) +
  geom_density()
```

The distribution of the predicted values for all 4 models are similar, with the same mean and variance. The means are also close to the mean of the Target_Win from our train data set. The variances are different because the test data has smaller sample size. All conclude that all 4 models produce plausible prediction values.

Based on above findings, the optimal model is

```{r}
summary(lm_win_eff_adj)
```
